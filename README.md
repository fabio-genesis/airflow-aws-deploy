# Airflow no Amazon ECS com Terraform

Este repositÃ³rio contÃ©m a infraestrutura como cÃ³digo (IaC) para implantar o Apache Airflow em contÃªineres gerenciados pela Amazon ECS (Elastic Container Service) usando Fargate, com o armazenamento de DAGs em um bucket S3 e todas as configuraÃ§Ãµes definidas usando Terraform.

## Arquitetura

A arquitetura deste projeto inclui:

- **Amazon ECS (Fargate)**: Para executar o Airflow em contÃªineres sem a necessidade de gerenciar servidores
  - **Webserver**: Interface web do Airflow (porta 8080)
  - **Scheduler**: Agendador de tarefas do Airflow
- **Amazon RDS (PostgreSQL)**: Para o banco de dados do Airflow
- **Amazon S3**: Para armazenar as DAGs do Airflow
- **Amazon VPC**: Com sub-redes pÃºblicas e privadas
- **Amazon ECR**: Para armazenar a imagem personalizada do Airflow
- **Application Load Balancer**: Para distribuir trÃ¡fego para o webserver

## Melhorias Recentes

Este repositÃ³rio foi atualizado com as seguintes correÃ§Ãµes crÃ­ticas:

### âœ… Problemas Corrigidos
- **Docker Entrypoint**: Corrigido loop infinito que impedia a inicializaÃ§Ã£o do Airflow
- **DependÃªncias**: Corrigido caminho do requirements.txt e adicionadas dependÃªncias essenciais
- **InicializaÃ§Ã£o do Banco**: Adicionada inicializaÃ§Ã£o automÃ¡tica do banco de dados do Airflow
- **SeparaÃ§Ã£o de ServiÃ§os**: Webserver e Scheduler agora executam em serviÃ§os ECS separados
- **Health Checks**: Adicionados health checks para monitoramento adequado
- **Tratamento de Erros**: Melhorado tratamento de erros no script de inicializaÃ§Ã£o
- **SincronizaÃ§Ã£o S3**: Aprimorada sincronizaÃ§Ã£o de DAGs do S3 com tratamento de erros

### ðŸ”§ Componentes Principais
- **Webserver**: ResponsÃ¡vel pela interface web (http://load-balancer-dns)
- **Scheduler**: ResponsÃ¡vel pelo agendamento e execuÃ§Ã£o de tarefas
- **SincronizaÃ§Ã£o S3**: SincronizaÃ§Ã£o automÃ¡tica de DAGs a cada 30 segundos

## PrÃ©-requisitos

- AWS CLI configurada
- Terraform instalado (v1.0.0+)
- Docker instalado
- PermissÃµes na AWS para criar e gerenciar os recursos necessÃ¡rios

## ConfiguraÃ§Ã£o e Deploy

### ConfiguraÃ§Ã£o Simples (Root Terraform)

Para uma configuraÃ§Ã£o mais simples e direta, vocÃª pode usar os arquivos terraform na raiz do projeto:

#### 1. Configurar VariÃ¡veis

```bash
# Copie o arquivo de exemplo
cp terraform.tfvars.example terraform.tfvars

# Edite com seus valores
nano terraform.tfvars
```

#### 2. Deploy da Infraestrutura

```bash
# Inicialize o Terraform
terraform init

# Revise o plano de execuÃ§Ã£o
terraform plan

# Execute o deploy
terraform apply
```

Este mÃ©todo criarÃ¡ automaticamente:
- âœ… **Bucket S3** com pastas `dags/` e `airflow-outputs/`
- âœ… **IAM Roles** com permissÃµes apropriadas para S3
- âœ… **RDS PostgreSQL** para metadados do Airflow
- âœ… **Security Groups** configurados corretamente
- âœ… **VPC e Subnets** usando a VPC padrÃ£o

### ConfiguraÃ§Ã£o Modular (Terraform Modules)

Para uma configuraÃ§Ã£o mais avanÃ§ada usando mÃ³dulos terraform, veja a pasta `terraform/`:

### 1. Clonar o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/airflow-ecs-terraform.git
cd airflow-ecs-terraform
```

### 2. Construir a imagem Docker do Airflow

```bash
cd docker
docker build -t airflow-on-ecs-fargate .
```

### 3. Autenticar no Amazon ECR

```bash
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 730335315247.dkr.ecr.us-east-1.amazonaws.com
```

### 4. Criar um repositÃ³rio no ECR (se ainda nÃ£o existir)

```bash
aws ecr create-repository --repository-name airflow-on-ecs-fargate --region us-east-1
```

### 5. Taguear e enviar a imagem para o ECR

```bash
docker tag airflow-on-ecs-fargate:latest 730335315247.dkr.ecr.us-east-1.amazonaws.com/airflow-on-ecs-fargate:latest
docker push 730335315247.dkr.ecr.us-east-1.amazonaws.com/airflow-on-ecs-fargate:latest
```

### 6. Criar o IAM Role para execuÃ§Ã£o das tarefas do ECS (se ainda nÃ£o existir)

```bash
# Criar a polÃ­tica que permite acesso ao S3
cat > airflow-s3-access-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket",
        "s3:PutObject",
        "s3:DeleteObject"
      ],
      "Resource": [
        "arn:aws:s3:::airflow-dev-test-install",
        "arn:aws:s3:::airflow-dev-test-install/*"
      ]
    }
  ]
}
EOF

aws iam create-policy --policy-name AirflowS3AccessPolicy --policy-document file://airflow-s3-access-policy.json

# Criar a role
aws iam create-role --role-name airflow-task-execution-role --assume-role-policy-document '{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ecs-tasks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}'

# Anexar polÃ­ticas Ã  role
aws iam attach-role-policy --role-name airflow-task-execution-role --policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy
aws iam attach-role-policy --role-name airflow-task-execution-role --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
aws iam attach-role-policy --role-name airflow-task-execution-role --policy-arn arn:aws:iam::$(aws sts get-caller-identity --query 'Account' --output text):policy/AirflowS3AccessPolicy
```

### 7. Implantar a infraestrutura com Terraform

Crie um arquivo `terraform.tfvars` com as variÃ¡veis necessÃ¡rias:

```hcl
db_name = "airflow"
aws_region = "us-east-1"
db_username = "airflow"
db_password = "airflow12345"
iam_role_ecs = "arn:aws:iam::730335315247:role/airflow-task-execution-role"
aws_ecr_repository = "730335315247.dkr.ecr.us-east-1.amazonaws.com/airflow-on-ecs-fargate:latest"
airflow_bucket_name = "airflow-dev-test-install"
```

Em seguida, execute:

```bash
cd ../terraform
terraform init
terraform plan -var-file="terraform.tfvars"
terraform apply -var-file="terraform.tfvars"
```

### 8. Acessar a interface do Airflow

ApÃ³s a implantaÃ§Ã£o bem-sucedida, a URL da interface do Airflow estarÃ¡ disponÃ­vel nos outputs do Terraform. Acesse a URL em um navegador para usar o Airflow. O endereÃ§o serÃ¡ exibido como `airflow_ui_url` nos outputs.

```bash
terraform output airflow_ui_url
```

### 9. Fazer upload das DAGs para o S3

```bash
# Copiar as DAGs para o bucket S3
aws s3 sync ./dags/ s3://airflow-dev-test-install/dags/
```

## AtualizaÃ§Ã£o das DAGs

Este sistema estÃ¡ configurado para sincronizar automaticamente as DAGs do bucket S3 com o diretÃ³rio local do Airflow a cada 30 segundos. Sempre que vocÃª adicionar ou modificar uma DAG:

1. FaÃ§a o upload da DAG para o bucket S3:
   ```bash
   aws s3 cp minha_dag.py s3://airflow-dev-test-install/dags/
   ```

2. A DAG serÃ¡ sincronizada automaticamente com o container do Airflow em atÃ© 30 segundos.

## Testando a DAG de Exemplo

O repositÃ³rio inclui uma DAG de exemplo (`dags/our_first_dag.py`) que demonstra:
- âœ… GeraÃ§Ã£o de dados sintÃ©ticos
- âœ… TransformaÃ§Ã£o de dados 
- âœ… Upload para S3 com tratamento de erros

### Para testar a DAG:

1. **Upload da DAG para S3:**
   ```bash
   aws s3 cp dags/our_first_dag.py s3://your-bucket-name/dags/
   ```

2. **Acesse a interface do Airflow** e procure pela DAG `daily_etl_pipeline_with_transform`

3. **Execute a DAG manualmente** para testar todas as funcionalidades

4. **Verifique os resultados no S3:**
   ```bash
   aws s3 ls s3://your-bucket-name/airflow-outputs/
   ```

### ConfiguraÃ§Ã£o para Desenvolvimento Local

Para testar localmente sem AWS, defina a variÃ¡vel de ambiente:
```bash
export SKIP_S3=true
```

Isso farÃ¡ com que a DAG funcione sem tentar acessar o S3.

## Como funciona a sincronizaÃ§Ã£o das DAGs

O sistema de sincronizaÃ§Ã£o automÃ¡tica funciona da seguinte maneira:

1. Quando o container do Airflow Ã© iniciado, o script `entrypoint.sh` executa uma primeira sincronizaÃ§Ã£o com o comando `aws s3 sync`.

2. Em seguida, um processo em background Ã© iniciado para verificar e sincronizar as DAGs a cada 30 segundos.

3. O comando `aws s3 sync` com a flag `--delete` garante que:
   - Novas DAGs sejam adicionadas
   - DAGs modificadas sejam atualizadas
   - DAGs removidas do S3 tambÃ©m sejam removidas do ambiente local

4. O Airflow verifica periodicamente o diretÃ³rio de DAGs para identificar alteraÃ§Ãµes.

## Limpeza

Para destruir toda a infraestrutura quando nÃ£o for mais necessÃ¡ria:

```bash
cd terraform
terraform destroy -var-file="terraform.tfvars"
```

## ObservaÃ§Ãµes

- A senha do banco de dados estÃ¡ em texto simples nas variÃ¡veis do Terraform.